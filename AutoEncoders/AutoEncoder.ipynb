{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5FB2AlndorI",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48v36PYAduBd",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0xZ26WOb16G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYP_RnWhzig",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R78tTQ2vegB0",
        "colab_type": "text"
      },
      "source": [
        "### Import the data from Git repo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsXD4aFteLCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_url = 'https://raw.githubusercontent.com/MarkIChen/deep-learning-unsupervised-learning/master/AutoEncoders/ml-1m/movies.dat'\n",
        "user_url = 'https://raw.githubusercontent.com/MarkIChen/deep-learning-unsupervised-learning/master/AutoEncoders/ml-1m/users.dat'\n",
        "rating_url = 'https://raw.githubusercontent.com/MarkIChen/deep-learning-unsupervised-learning/master/AutoEncoders/ml-1m/ratings.dat'\n",
        "\n",
        "movies = pd.read_csv(movie_url, sep= '::', header = None, engine= 'python', encoding= 'latin-1')\n",
        "users = pd.read_csv(user_url, sep= '::', header = None, engine= 'python', encoding= 'latin-1')\n",
        "rating = pd.read_csv(rating_url, sep= '::', header= None, engine= 'python', encoding= 'latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKMOZxZ2h51k",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the traing set and the test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJS3V0NmgYJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u1_train_url = 'https://raw.githubusercontent.com/MarkIChen/deep-learning-unsupervised-learning/master/AutoEncoders/ml-100k/u1.base'\n",
        "u1_test_url = 'https://raw.githubusercontent.com/MarkIChen/deep-learning-unsupervised-learning/master/AutoEncoders/ml-100k/u1.test'\n",
        "\n",
        "training_set = pd.read_csv(u1_train_url, delimiter= '\\t')\n",
        "test_set = pd.read_csv(u1_test_url, delimiter= '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2NdjmYjHKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "15f70979-1e0a-4cc4-e6cf-cbb7e1dd05fb"
      },
      "source": [
        "training_set.head"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of          1   1.1  5  874965758\n",
              "0        1     2  3  876893171\n",
              "1        1     3  4  878542960\n",
              "2        1     4  3  876893119\n",
              "3        1     5  3  889751712\n",
              "4        1     7  4  875071561\n",
              "...    ...   ... ..        ...\n",
              "79994  943  1067  2  875501756\n",
              "79995  943  1074  4  888640250\n",
              "79996  943  1188  3  888640250\n",
              "79997  943  1228  3  888640275\n",
              "79998  943  1330  3  888692465\n",
              "\n",
              "[79999 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6APKLRBgjKtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "d9854ce4-1a52-4097-fa74-0dcfcc491f22"
      },
      "source": [
        "test_set.head"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of          1     6  5  887431973\n",
              "0        1    10  3  875693118\n",
              "1        1    12  5  878542960\n",
              "2        1    14  5  874965706\n",
              "3        1    17  3  875073198\n",
              "4        1    20  4  887431883\n",
              "...    ...   ... ..        ...\n",
              "19994  458   648  4  886395899\n",
              "19995  458  1101  4  886397931\n",
              "19996  459   934  3  879563639\n",
              "19997  460    10  3  882912371\n",
              "19998  462   682  5  886365231\n",
              "\n",
              "[19999 rows x 4 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSFJolcujFAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = np.array(training_set, dtype= 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXkaAGWjhOC",
        "colab_type": "text"
      },
      "source": [
        "### Getting the number of users and movies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycdfZIzNjWWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_users = int(max( max(training_set[:, 0]), max(test_set[:, 0])))\n",
        "nb_movies = int(max( max(training_set[:, 1]), max(test_set[:, 1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjfreJNYkO0X",
        "colab_type": "text"
      },
      "source": [
        "### Converting the data into an array with users in line and movies in columns(one hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwVlRKA2jgiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_users in range(1, nb_users + 1):\n",
        "    id_movies = data[:, 1][data[:, 0] == id_users]\n",
        "    id_ratings = data[:, 2][data[:, 0] == id_users]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V1l_9ZVl84d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP4WmtI19GX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "776bd5e9-fedf-4c54-bf19-31b333b35c41"
      },
      "source": [
        "training_set"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
              "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 5., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stcs9GvT9LRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "7f3078d0-47b7-4664-bf30-358c613cabcf"
      },
      "source": [
        "test_set"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6pMgS8Dq8Bk",
        "colab_type": "text"
      },
      "source": [
        "### Converting the data into  Torch tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qSr4n3q4BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvnoWBFIrruZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d1b98220-fd2f-400c-98ad-80444bf25810"
      },
      "source": [
        "training_set"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
              "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 5., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO5ouxiQsD47",
        "colab_type": "text"
      },
      "source": [
        "## Creating the architecture of the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87gSluPTr43r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SAE(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(SAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(nb_movies, 20)\n",
        "    self.fc2 = nn.Linear(20, 10)\n",
        "    self.fc3 = nn.Linear(10, 20)\n",
        "    self.fc4 = nn.Linear(20, nb_movies)\n",
        "    self.activation = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.activation(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhtRg3rBzDnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJjFw2E30AJ_",
        "colab_type": "text"
      },
      "source": [
        "## Training the SAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjFnYkXtyB_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86baec9a-a49f-4e38-94f0-ad5a34740ac4"
      },
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "  for id_user in range(nb_users):\n",
        "    # increasing an additional dimension\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = input.clone()\n",
        "    # if the user did not rate any movie, we don't need to cal. it.\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input) # which function?\n",
        "      # To make sure that back-prob. will not update the target layer.\n",
        "      target.require_grad = False\n",
        "      # if the user did not rate the movie the prediction should be zero.\n",
        "      output[target == 0] = 0\n",
        "      loss = criterion(output, target)\n",
        "      # We only concern about the none zero rating\n",
        "      # Adding a small number is a math skill to avoid the sum is zero.\n",
        "      mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "      # deciding whether adding or decreasing the weights\n",
        "      loss.backward()\n",
        "      train_loss += np.sqrt(loss.data * mean_corrector)\n",
        "      s += 1.\n",
        "      # deciding the amount of change\n",
        "      optimizer.step()\n",
        "  print('epcoh '+str(epoch)+' loss: '+str(train_loss/s))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epcoh 1 loss: tensor(1.7623)\n",
            "epcoh 2 loss: tensor(1.0964)\n",
            "epcoh 3 loss: tensor(1.0533)\n",
            "epcoh 4 loss: tensor(1.0385)\n",
            "epcoh 5 loss: tensor(1.0307)\n",
            "epcoh 6 loss: tensor(1.0267)\n",
            "epcoh 7 loss: tensor(1.0238)\n",
            "epcoh 8 loss: tensor(1.0220)\n",
            "epcoh 9 loss: tensor(1.0206)\n",
            "epcoh 10 loss: tensor(1.0199)\n",
            "epcoh 11 loss: tensor(1.0191)\n",
            "epcoh 12 loss: tensor(1.0184)\n",
            "epcoh 13 loss: tensor(1.0179)\n",
            "epcoh 14 loss: tensor(1.0175)\n",
            "epcoh 15 loss: tensor(1.0174)\n",
            "epcoh 16 loss: tensor(1.0170)\n",
            "epcoh 17 loss: tensor(1.0167)\n",
            "epcoh 18 loss: tensor(1.0165)\n",
            "epcoh 19 loss: tensor(1.0164)\n",
            "epcoh 20 loss: tensor(1.0163)\n",
            "epcoh 21 loss: tensor(1.0161)\n",
            "epcoh 22 loss: tensor(1.0160)\n",
            "epcoh 23 loss: tensor(1.0161)\n",
            "epcoh 24 loss: tensor(1.0159)\n",
            "epcoh 25 loss: tensor(1.0158)\n",
            "epcoh 26 loss: tensor(1.0156)\n",
            "epcoh 27 loss: tensor(1.0155)\n",
            "epcoh 28 loss: tensor(1.0152)\n",
            "epcoh 29 loss: tensor(1.0128)\n",
            "epcoh 30 loss: tensor(1.0113)\n",
            "epcoh 31 loss: tensor(1.0103)\n",
            "epcoh 32 loss: tensor(1.0082)\n",
            "epcoh 33 loss: tensor(1.0075)\n",
            "epcoh 34 loss: tensor(1.0043)\n",
            "epcoh 35 loss: tensor(1.0028)\n",
            "epcoh 36 loss: tensor(1.0000)\n",
            "epcoh 37 loss: tensor(0.9986)\n",
            "epcoh 38 loss: tensor(0.9970)\n",
            "epcoh 39 loss: tensor(0.9941)\n",
            "epcoh 40 loss: tensor(0.9918)\n",
            "epcoh 41 loss: tensor(0.9940)\n",
            "epcoh 42 loss: tensor(0.9857)\n",
            "epcoh 43 loss: tensor(0.9885)\n",
            "epcoh 44 loss: tensor(0.9853)\n",
            "epcoh 45 loss: tensor(0.9848)\n",
            "epcoh 46 loss: tensor(0.9849)\n",
            "epcoh 47 loss: tensor(0.9872)\n",
            "epcoh 48 loss: tensor(0.9808)\n",
            "epcoh 49 loss: tensor(0.9857)\n",
            "epcoh 50 loss: tensor(0.9793)\n",
            "epcoh 51 loss: tensor(0.9773)\n",
            "epcoh 52 loss: tensor(0.9723)\n",
            "epcoh 53 loss: tensor(0.9719)\n",
            "epcoh 54 loss: tensor(0.9698)\n",
            "epcoh 55 loss: tensor(0.9773)\n",
            "epcoh 56 loss: tensor(0.9779)\n",
            "epcoh 57 loss: tensor(0.9744)\n",
            "epcoh 58 loss: tensor(0.9746)\n",
            "epcoh 59 loss: tensor(0.9767)\n",
            "epcoh 60 loss: tensor(0.9741)\n",
            "epcoh 61 loss: tensor(0.9710)\n",
            "epcoh 62 loss: tensor(0.9683)\n",
            "epcoh 63 loss: tensor(0.9732)\n",
            "epcoh 64 loss: tensor(0.9726)\n",
            "epcoh 65 loss: tensor(0.9676)\n",
            "epcoh 66 loss: tensor(0.9626)\n",
            "epcoh 67 loss: tensor(0.9645)\n",
            "epcoh 68 loss: tensor(0.9590)\n",
            "epcoh 69 loss: tensor(0.9575)\n",
            "epcoh 70 loss: tensor(0.9546)\n",
            "epcoh 71 loss: tensor(0.9566)\n",
            "epcoh 72 loss: tensor(0.9533)\n",
            "epcoh 73 loss: tensor(0.9525)\n",
            "epcoh 74 loss: tensor(0.9510)\n",
            "epcoh 75 loss: tensor(0.9494)\n",
            "epcoh 76 loss: tensor(0.9504)\n",
            "epcoh 77 loss: tensor(0.9503)\n",
            "epcoh 78 loss: tensor(0.9482)\n",
            "epcoh 79 loss: tensor(0.9480)\n",
            "epcoh 80 loss: tensor(0.9447)\n",
            "epcoh 81 loss: tensor(0.9457)\n",
            "epcoh 82 loss: tensor(0.9446)\n",
            "epcoh 83 loss: tensor(0.9445)\n",
            "epcoh 84 loss: tensor(0.9418)\n",
            "epcoh 85 loss: tensor(0.9425)\n",
            "epcoh 86 loss: tensor(0.9416)\n",
            "epcoh 87 loss: tensor(0.9412)\n",
            "epcoh 88 loss: tensor(0.9398)\n",
            "epcoh 89 loss: tensor(0.9404)\n",
            "epcoh 90 loss: tensor(0.9408)\n",
            "epcoh 91 loss: tensor(0.9439)\n",
            "epcoh 92 loss: tensor(0.9441)\n",
            "epcoh 93 loss: tensor(0.9436)\n",
            "epcoh 94 loss: tensor(0.9424)\n",
            "epcoh 95 loss: tensor(0.9422)\n",
            "epcoh 96 loss: tensor(0.9405)\n",
            "epcoh 97 loss: tensor(0.9404)\n",
            "epcoh 98 loss: tensor(0.9388)\n",
            "epcoh 99 loss: tensor(0.9391)\n",
            "epcoh 100 loss: tensor(0.9380)\n",
            "epcoh 101 loss: tensor(0.9386)\n",
            "epcoh 102 loss: tensor(0.9365)\n",
            "epcoh 103 loss: tensor(0.9369)\n",
            "epcoh 104 loss: tensor(0.9353)\n",
            "epcoh 105 loss: tensor(0.9364)\n",
            "epcoh 106 loss: tensor(0.9346)\n",
            "epcoh 107 loss: tensor(0.9362)\n",
            "epcoh 108 loss: tensor(0.9345)\n",
            "epcoh 109 loss: tensor(0.9354)\n",
            "epcoh 110 loss: tensor(0.9333)\n",
            "epcoh 111 loss: tensor(0.9348)\n",
            "epcoh 112 loss: tensor(0.9326)\n",
            "epcoh 113 loss: tensor(0.9335)\n",
            "epcoh 114 loss: tensor(0.9317)\n",
            "epcoh 115 loss: tensor(0.9327)\n",
            "epcoh 116 loss: tensor(0.9310)\n",
            "epcoh 117 loss: tensor(0.9323)\n",
            "epcoh 118 loss: tensor(0.9303)\n",
            "epcoh 119 loss: tensor(0.9310)\n",
            "epcoh 120 loss: tensor(0.9295)\n",
            "epcoh 121 loss: tensor(0.9304)\n",
            "epcoh 122 loss: tensor(0.9292)\n",
            "epcoh 123 loss: tensor(0.9286)\n",
            "epcoh 124 loss: tensor(0.9278)\n",
            "epcoh 125 loss: tensor(0.9281)\n",
            "epcoh 126 loss: tensor(0.9274)\n",
            "epcoh 127 loss: tensor(0.9280)\n",
            "epcoh 128 loss: tensor(0.9272)\n",
            "epcoh 129 loss: tensor(0.9275)\n",
            "epcoh 130 loss: tensor(0.9264)\n",
            "epcoh 131 loss: tensor(0.9269)\n",
            "epcoh 132 loss: tensor(0.9256)\n",
            "epcoh 133 loss: tensor(0.9261)\n",
            "epcoh 134 loss: tensor(0.9248)\n",
            "epcoh 135 loss: tensor(0.9255)\n",
            "epcoh 136 loss: tensor(0.9245)\n",
            "epcoh 137 loss: tensor(0.9244)\n",
            "epcoh 138 loss: tensor(0.9237)\n",
            "epcoh 139 loss: tensor(0.9241)\n",
            "epcoh 140 loss: tensor(0.9225)\n",
            "epcoh 141 loss: tensor(0.9233)\n",
            "epcoh 142 loss: tensor(0.9223)\n",
            "epcoh 143 loss: tensor(0.9228)\n",
            "epcoh 144 loss: tensor(0.9221)\n",
            "epcoh 145 loss: tensor(0.9220)\n",
            "epcoh 146 loss: tensor(0.9213)\n",
            "epcoh 147 loss: tensor(0.9213)\n",
            "epcoh 148 loss: tensor(0.9215)\n",
            "epcoh 149 loss: tensor(0.9214)\n",
            "epcoh 150 loss: tensor(0.9207)\n",
            "epcoh 151 loss: tensor(0.9209)\n",
            "epcoh 152 loss: tensor(0.9202)\n",
            "epcoh 153 loss: tensor(0.9195)\n",
            "epcoh 154 loss: tensor(0.9196)\n",
            "epcoh 155 loss: tensor(0.9200)\n",
            "epcoh 156 loss: tensor(0.9193)\n",
            "epcoh 157 loss: tensor(0.9192)\n",
            "epcoh 158 loss: tensor(0.9190)\n",
            "epcoh 159 loss: tensor(0.9189)\n",
            "epcoh 160 loss: tensor(0.9185)\n",
            "epcoh 161 loss: tensor(0.9190)\n",
            "epcoh 162 loss: tensor(0.9183)\n",
            "epcoh 163 loss: tensor(0.9185)\n",
            "epcoh 164 loss: tensor(0.9182)\n",
            "epcoh 165 loss: tensor(0.9177)\n",
            "epcoh 166 loss: tensor(0.9175)\n",
            "epcoh 167 loss: tensor(0.9182)\n",
            "epcoh 168 loss: tensor(0.9174)\n",
            "epcoh 169 loss: tensor(0.9174)\n",
            "epcoh 170 loss: tensor(0.9168)\n",
            "epcoh 171 loss: tensor(0.9174)\n",
            "epcoh 172 loss: tensor(0.9165)\n",
            "epcoh 173 loss: tensor(0.9169)\n",
            "epcoh 174 loss: tensor(0.9164)\n",
            "epcoh 175 loss: tensor(0.9162)\n",
            "epcoh 176 loss: tensor(0.9160)\n",
            "epcoh 177 loss: tensor(0.9159)\n",
            "epcoh 178 loss: tensor(0.9162)\n",
            "epcoh 179 loss: tensor(0.9164)\n",
            "epcoh 180 loss: tensor(0.9158)\n",
            "epcoh 181 loss: tensor(0.9165)\n",
            "epcoh 182 loss: tensor(0.9156)\n",
            "epcoh 183 loss: tensor(0.9153)\n",
            "epcoh 184 loss: tensor(0.9147)\n",
            "epcoh 185 loss: tensor(0.9155)\n",
            "epcoh 186 loss: tensor(0.9148)\n",
            "epcoh 187 loss: tensor(0.9147)\n",
            "epcoh 188 loss: tensor(0.9143)\n",
            "epcoh 189 loss: tensor(0.9150)\n",
            "epcoh 190 loss: tensor(0.9143)\n",
            "epcoh 191 loss: tensor(0.9143)\n",
            "epcoh 192 loss: tensor(0.9135)\n",
            "epcoh 193 loss: tensor(0.9135)\n",
            "epcoh 194 loss: tensor(0.9130)\n",
            "epcoh 195 loss: tensor(0.9141)\n",
            "epcoh 196 loss: tensor(0.9135)\n",
            "epcoh 197 loss: tensor(0.9139)\n",
            "epcoh 198 loss: tensor(0.9132)\n",
            "epcoh 199 loss: tensor(0.9134)\n",
            "epcoh 200 loss: tensor(0.9127)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlySEhmJ8L5X",
        "colab_type": "text"
      },
      "source": [
        "## Testing the SAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N9dPU435_d1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a80af5da-4085-4f1f-edc8-1aa370ee4f70"
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "  if torch.sum(target.data > 0) > 0 :\n",
        "    output = sae(input)\n",
        "    target.require_grad = False\n",
        "    output[target == 0] = 0\n",
        "    loss = criterion(output, target)\n",
        "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "    test_loss += np.sqrt(loss.data * mean_corrector)\n",
        "    s+= 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: tensor(0.9511)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}